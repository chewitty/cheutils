##
# Project properties
##
project.namespace=cheutils
project.root.dir=./
project.data.dir=./data/
project.output.dir=./output/
# properties handlers
project.properties.proj_handler={'name': 'ProjectTreeProperties', 'package': 'cheutils', }
project.properties.data_handler={'name': 'DataPropertiesHandler', 'package': 'cheutils', }
project.properties.model_handler={'name': 'ModelProperties', 'package': 'cheutils', }
# SQLite DB - used for selected caching for efficiency
project.sqlite3.db=cheutils_sqlite.db
project.dataset.list=[X_train.csv, X_test.csv, y_train.csv, y_test.csv]
# estimator configuration: default parameters are those not necessarily included for any tuning or optimization
# but are useful for instantiating instances of the estimator; all others in the estimator params_grid are
# candidates for any optimization. If no default parameters are needed simply ignore or set default_params value to None
project.models.supported={'xgb_boost': {'name': 'XGBRegressor', 'package': 'xgboost', 'default_params': None, }, \
  'random_forest': {'name': 'RandomForestRegressor', 'package': 'sklearn.ensemble', 'default_params': None, }, \
  'lasso': {'name': 'Lasso', 'package': 'sklearn.linear_model'}, 'default_params': {'alpha': 0.10}, }
# selected estimator parameter grid options - these are included in any tuning or model optimization
model.params_grid.xgb_boost={'learning_rate': {'type': float, 'start': 0.0, 'end': 1.0, 'num': 10}, 'subsample': {'type': float, 'start': 0.0, 'end': 1.0, 'num': 10}, 'min_child_weight': {'type': float, 'start': 0.1, 'end': 1.0, 'num': 10}, 'n_estimators': {'type': int, 'start': 10, 'end': 400, 'num': 10}, 'max_depth': {'type': int, 'start': 3, 'end': 17, 'num': 5}, 'colsample_bytree': {'type': float, 'start': 0.0, 'end': 1.0, 'num': 5}, 'gamma': {'type': float, 'start': 0.0, 'end': 1.0, 'num': 5}, 'reg_alpha': {'type': float, 'start': 0.0, 'end': 1.0, 'num': 5}, }
model.params_grid.random_forest={'min_samples_leaf': {'type': int, 'start': 1, 'end': 60, 'num': 5}, 'max_features': {'type': int, 'start': 5, 'end': 1001, 'num': 10}, 'max_depth': {'type': int, 'start': 5, 'end': 31, 'num': 6}, 'n_estimators': {'type': int, 'start': 5, 'end': 201, 'num': 10}, 'min_samples_split': {'type': int, 'start': 2, 'end': 21, 'num': 5}, 'max_leaf_nodes': {'type': int, 'start': 5, 'end': 401, 'num': 10}, }
model.baseline.model_option=lasso
model.active.model_option=xgb_boost
# hyperparameter search algorith options supported: hyperopt with cross-validation and Scikit-Optimize
project.hyperparam.searches=['hyperoptcv', 'skoptimize']
model.active.n_iters=200
model.active.n_trials=10
model.narrow_grid.scaling_factor=0.20
model.narrow_grid.scaling_factors={'start': 0.1, 'end': 1.0, 'steps': 10}
model.find_optimal.grid_resolution=False
model.find_optimal.grid_resolution.with_cv=False
model.grid_resolutions.sample={'start': 1, 'end': 21, 'step': 1}
model.active.grid_resolution=7
model.cross_val.num_folds=3
model.active.n_jobs=-1
model.cross_val.scoring=neg_mean_squared_error
model.active.random_seed=100
model.active.trial_timeout=60
model.hyperopt.algos={'rand.suggest': 0.05, 'tpe.suggest': 0.75, 'anneal.suggest': 0.20, }
# transformers - defined as a dictionary of pipelines containing dictionaries of transformers
# note that each pipeline is mapped to a set of columns, and all transformers in a pipeline act on the set of columns
model.feature.scalers=[{'pipeline_name': 'scaler_pipeline', 'transformers': [{'name': 'scaler_tf', 'module': 'StandardScaler', 'package': 'sklearn.preprocessing', 'params': None, }, ], 'columns': ['col1_label', 'col2_label']}, ]
model.feature.encoders=[{'pipeline_name': 'encoder_pipeline', 'transformers': [{'name': 'encoder_tf', 'module': 'OneHotEncoder', 'package': 'sklearn.preprocessing', 'params': None, }, ], 'columns': ['col1_label', 'col2_label']}, ]
model.feature.binarizers=[{'pipeline_name': 'binarizers_pipeline', 'transformers': [{'name': 'binarizer_tf', 'module': 'Binarizer', 'package': 'sklearn.preprocessing', 'params': {'threshold': 0.5, }, }, ], 'columns': ['col1_label', 'col2_label']}, ]
# transformers - defined as a dictionary of pipelines containing dictionaries of transformers
# note that each pipeline is mapped to a set of columns, and all transformers in a pipeline act on the set of columns
model.target.encoder={'pipeline_name': 'target_enc_pipeline', 'target_encoder': {'name': 'target_enc_tf', 'module': 'SelectiveTargetEncoder', 'package': 'sklearn.preprocessing', 'params': {'target_type': 'auto', 'smooth': 'auto', 'cv': 5, 'shuffle': True, }, }, 'columns': ['col1_label', 'col2_label'], }
# configure feature selection transformers
model.feature.selectors={\
  'selector1': {'module': 'SelectFromModel', 'package': 'sklearn.feature_selection', 'params': {'threshold': 'median', }, }, \
  'selector2': {'module': 'RFE', 'package': 'sklearn.feature_selection', 'params': {'n_features_to_select': 0.25, }, }, \
  }
model.feat_selection.passthrough=True
model.feat_selection.selector=selector1
# use the following once feature selection has been settled - extracted from feature selection to use going forward
# an example situation where this is helpful is during tuning when the feature selection has been settled beforehand.
# It means feature selection does not have to be a pipeline step, as that can introduce errors if cross-validation is as
# different folds may select different feature subsets - we desire a consistent set of features for each fold.
# So, set use_selected=False, run test pipeline notebook to generate list of selected features; then set use_selected=True
# that enables the pipeline to simply take advantage of the selected features (which should be copied and pasted below).
model.feat_selection.use_selected=False
# once feature selection is done the output is copied and pasted below to be optionally used as selected
model.feat_selection.selected=['col1', 'col2', ]
# global winsorize default limits or specify desired property and use accordingly
func.winsorize.limits=[0.05, 0.05]
